{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: '[WinError 127] No se encontrÃ³ el proceso especificado'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tranform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = torchvision.datasets.ImageFolder(root='Shapes', transform=my_tranform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Circles -> 0\n",
    "Stars -> 1\n",
    "Triangles -> 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_loader = torch.utils.data.DataLoader(dataset, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8*len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_loader = torch.utils.data.DataLoader(train_dataset, \\\n",
    "                                             batch_size=64,\n",
    "                                             shuffle=True)\n",
    "test_dataset_loader = torch.utils.data.DataLoader(test_dataset, \\\n",
    "                                             batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.) tensor(0.6946) tensor(0.1746)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sergi\\anaconda3\\lib\\site-packages\\PIL\\Image.py:996: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.) tensor(0.) tensor(0.7189) tensor(0.1918)\n",
      "tensor(1.) tensor(0.) tensor(0.7207) tensor(0.1667)\n",
      "tensor(1.) tensor(0.) tensor(0.7178) tensor(0.1788)\n",
      "tensor(1.) tensor(0.) tensor(0.7269) tensor(0.1968)\n",
      "tensor(1.) tensor(0.) tensor(0.7156) tensor(0.1630)\n",
      "tensor(1.) tensor(0.) tensor(0.7138) tensor(0.1740)\n",
      "tensor(1.) tensor(0.) tensor(0.7247) tensor(0.1767)\n",
      "Epoch: 0\t Accuracy: 0.35729387402534485\n",
      "tensor(1.) tensor(0.) tensor(0.7296) tensor(0.1894)\n",
      "tensor(1.) tensor(0.) tensor(0.7233) tensor(0.1772)\n",
      "tensor(1.) tensor(0.) tensor(0.7280) tensor(0.1783)\n",
      "tensor(1.) tensor(0.) tensor(0.6812) tensor(0.1754)\n",
      "tensor(1.) tensor(0.) tensor(0.7254) tensor(0.1801)\n",
      "tensor(1.) tensor(0.) tensor(0.7066) tensor(0.1633)\n",
      "tensor(1.) tensor(0.) tensor(0.7032) tensor(0.1757)\n",
      "tensor(1.) tensor(0.) tensor(0.7527) tensor(0.1838)\n",
      "Epoch: 1\t Accuracy: 0.37632134556770325\n",
      "tensor(1.) tensor(0.) tensor(0.7069) tensor(0.1710)\n",
      "tensor(1.) tensor(0.) tensor(0.7253) tensor(0.1798)\n",
      "tensor(1.) tensor(0.) tensor(0.7135) tensor(0.1764)\n",
      "tensor(1.) tensor(0.) tensor(0.7082) tensor(0.1673)\n",
      "tensor(1.) tensor(0.) tensor(0.7159) tensor(0.1743)\n",
      "tensor(1.) tensor(0.) tensor(0.7085) tensor(0.1908)\n",
      "tensor(1.) tensor(0.) tensor(0.7329) tensor(0.1854)\n",
      "tensor(1.) tensor(0.) tensor(0.7170) tensor(0.1829)\n",
      "Epoch: 2\t Accuracy: 0.4482029676437378\n",
      "tensor(1.) tensor(0.) tensor(0.7263) tensor(0.1805)\n",
      "tensor(1.) tensor(0.) tensor(0.6954) tensor(0.1622)\n",
      "tensor(1.) tensor(0.) tensor(0.7160) tensor(0.1862)\n",
      "tensor(1.) tensor(0.) tensor(0.7121) tensor(0.1933)\n",
      "tensor(1.) tensor(0.) tensor(0.6971) tensor(0.1726)\n",
      "tensor(1.) tensor(0.) tensor(0.7198) tensor(0.1676)\n",
      "tensor(1.) tensor(0.) tensor(0.7338) tensor(0.1814)\n",
      "tensor(1.) tensor(0.) tensor(0.7446) tensor(0.1768)\n",
      "Epoch: 3\t Accuracy: 0.5496828556060791\n",
      "tensor(1.) tensor(0.) tensor(0.7162) tensor(0.1693)\n",
      "tensor(1.) tensor(0.) tensor(0.6918) tensor(0.1734)\n",
      "tensor(1.) tensor(0.) tensor(0.7272) tensor(0.1670)\n",
      "tensor(1.) tensor(0.) tensor(0.7227) tensor(0.1808)\n",
      "tensor(1.) tensor(0.) tensor(0.7037) tensor(0.1796)\n",
      "tensor(1.) tensor(0.) tensor(0.7222) tensor(0.1942)\n",
      "tensor(1.) tensor(0.) tensor(0.7108) tensor(0.1728)\n",
      "tensor(1.) tensor(0.) tensor(0.7600) tensor(0.1933)\n",
      "Epoch: 4\t Accuracy: 0.6321353316307068\n",
      "tensor(1.) tensor(0.) tensor(0.7129) tensor(0.1927)\n",
      "tensor(1.) tensor(0.) tensor(0.7172) tensor(0.1910)\n",
      "tensor(1.) tensor(0.) tensor(0.7136) tensor(0.1547)\n",
      "tensor(1.) tensor(0.) tensor(0.7270) tensor(0.1852)\n",
      "tensor(1.) tensor(0.) tensor(0.7321) tensor(0.1757)\n",
      "tensor(1.) tensor(0.) tensor(0.7110) tensor(0.1935)\n",
      "tensor(1.) tensor(0.) tensor(0.7156) tensor(0.1455)\n",
      "tensor(1.) tensor(0.) tensor(0.6707) tensor(0.1820)\n",
      "Epoch: 5\t Accuracy: 0.7145877480506897\n",
      "tensor(1.) tensor(0.) tensor(0.6996) tensor(0.1812)\n",
      "tensor(1.) tensor(0.) tensor(0.7185) tensor(0.1860)\n",
      "tensor(1.) tensor(0.) tensor(0.7003) tensor(0.1736)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m hot_labels \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mone_hot(labels, \u001b[39m3\u001b[39m)\u001b[39m.\u001b[39mto(torch\u001b[39m.\u001b[39mfloat32)\n\u001b[0;32m     44\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(hot_labels, hot_predictions)\n\u001b[1;32m---> 45\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     46\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39mitem())\n\u001b[0;32m     47\u001b[0m opt\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\sergi\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\sergi\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "nb_neurons = 64-5-5-5-5+4\n",
    "nb_kernel = 64\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.layer1 = nn.Conv2d(3, 8, 5)\n",
    "        # self.layer2 = nn.Conv2d(8, 16, 5)\n",
    "        # self.layer3 = nn.Conv2d(16, 32, 5)\n",
    "        # self.layer4 = nn.Conv2d(32, 64, 5)\n",
    "        # self.layer5 = nn.Linear((nb_neurons**2)*nb_kernel, 3)\n",
    "        self.layer1 = nn.Conv2d(3, 8, 5, padding = 2)\n",
    "        self.layer2 = nn.Conv2d(8, 16, 5, padding = 2)\n",
    "        self.layer3 = nn.Conv2d(16, 32, 5, padding = 2)\n",
    "        self.layer4 = nn.Conv2d(32, 64, 5, padding = 2)\n",
    "        self.layer5 = nn.Linear(64*64*64, 3)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        l = self.relu(self.layer1(x))\n",
    "        l = self.relu(self.layer2(l))\n",
    "        l = self.relu(self.layer3(l))\n",
    "        l = self.relu(self.layer4(l))\n",
    "        l = l.view(-1,64*64*64)\n",
    "        # l = l.view(-1,(nb_neurons**2)*nb_kernel)\n",
    "        return F.softmax(self.layer5(l), dim=1)\n",
    "\n",
    "net = Net()\n",
    "opt = optim.Adam(net.parameters(), 0.0001)\n",
    "loss_fn = nn.MSELoss()\n",
    "losses = []\n",
    "accuracies = [0.1]\n",
    "\n",
    "for epoch in range(100):\n",
    "    total_correct = 0\n",
    "    total = train_size\n",
    "    for images, labels in train_dataset_loader:\n",
    "        print(images.max(), images.min(), images.mean(), images.std())\n",
    "        hot_predictions = net(images)\n",
    "        predictions = hot_predictions.argmax(axis=1)\n",
    "        nb_correct = sum(predictions == labels)\n",
    "        total_correct += nb_correct\n",
    "        hot_labels = F.one_hot(labels, 3).to(torch.float32)\n",
    "        loss = loss_fn(hot_labels, hot_predictions)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    acc = total_correct/total\n",
    "    accuracies.append(acc.item())\n",
    "    print(f\"Epoch: {epoch}\\t Accuracy: {acc}\")\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(losses)\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(accuracies, \"*r\")\n",
    "plt.ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9495798349380493\n"
     ]
    }
   ],
   "source": [
    "total_correct = 0\n",
    "total = test_size\n",
    "for images, labels in test_dataset_loader:\n",
    "    hot_predictions = net(images)\n",
    "    predictions = hot_predictions.argmax(axis=1)\n",
    "    nb_correct = sum(predictions == labels)\n",
    "    total_correct += nb_correct\n",
    "    hot_labels = F.one_hot(labels, 3).to(torch.float32)\n",
    "    loss = loss_fn(hot_labels, hot_predictions)\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    opt.zero_grad()\n",
    "acc = total_correct/total\n",
    "print(f\"Accuracy: {acc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55, 3, 64, 64])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input = torch.ones(1, 3, 64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model in a onnx file\n",
    "torch.onnx.export(net, dummy_input, 'Test_model.onnx', export_params = True, verbose = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
